# Project 1: Exploratory Data Analysis

## Project Description:
This project analyzes awarded contract data from the City of Vancouver (including the Library, Police Board, and Park Board). The goal is to uncover patterns in vendor bid success rates, focusing on how bid amounts affect the likelihood of winning contracts.

## Project Title:
**Exploratory Data Analysis on Vancouver Group Contracts Dataset**

## Objective:
Analyze the relationship between bid amounts and the success rate of awarded contracts for the Vancouver Group. The analysis focuses on identifying key factors influencing vendor success by examining bid amounts, vendor bids, and awarded outcomes.

## Dataset:
The dataset contains 646 entries of contract bids made by the contractors or vendors for the projects posted publicly by the City of Vancouver group with the following features:
- **Bid Number**: Unique identifier for each bid
- **Bid Type**: Type of bid (e.g., Request for Proposal)
- **Description**: Description of the contract
- **Award Date**: Date when the contract was awarded
- **Vendor Name**: Name of the vendor or contractor
- **Bid Amount**: Monetary value of the bid (may have missing values)
- **Awarded**: Status indicating if the bid was awarded (Yes/No)

## Methodology:

### 1. Data Ingestion:
- Uploaded raw data into the AWS S3 bucket (cov-raw-roh) dedicated to raw data.
- Organized data by folder structure based on ingestion year and quarter.
- Configured the S3 storage class as Standard for frequent access, with lifecycle rules to transition to Glacier Flexible Retrieval after 90 days.

### 2. Data Profiling:
- Used AWS Glue DataBrew to profile the dataset and assess data quality.
- Identified missing values and duplicate rows.
- Stored profiling results in dedicated subfolders in the S3 bucket (cov-trf-roh).

### 3. Data Cleaning:
- Standardized column names by replacing space with underscores.
- Filled the missing values with NULL values.
- Dropped unnecessary columns.
- Stored cleaned data in CSV format for users and Parquet format for system processing.
- Results of data cleaning are stored in a separate dedicated S3 bucket (cov-trf-roh).

### 4. Data Pipeline Design:
- Designed a pipeline using AWS Glue with nodes for filtering, aggregating, and merging datasets.
- Extracted Awarded contracts and Bid Amount for analysis.
- Aggregated data to get the count of awarded contracts and total bids for each bid number and vendor name.
- Calculated contract awarded rate.
- Results of the exploratory analysis are stored in a separate bucket (cov-cur-roh).

### 5. Data Analysis:
- The dataset was analyzed to identify the link between Bid Amount and contract award likelihood.
- Contracts with specified bid amounts were compared against those with missing bid amounts to identify success rate trends.

## Tools & Technologies:
- **AWS S3**: Data Storage
- **AWS Glue DataBrew**: Data Profiling and Data Cleaning
- **AWS Glue Studio**: ETL Pipeline

## Deliverables:
- AWS Glue Data Pipeline for processing, aggregating, and merging the dataset.
- Data Profiling Report generated by AWS Glue DataBrew, highlighting data quality issues.
- Cleaned Data Files stored in S3 buckets in both CSV and Parquet formats.
- Visualization of the correlation between the bid amount and contract award rate.

## Insights & Findings:
- There is a strong correlation between specifying a Bid Amount and a 95% success rate for contract awards.
- A negative correlation exists between missing Bid Amounts and lower contract success rates.
- Vendors who included Bid Amounts in their bids show a positive correlation with higher success rates.
- The high percentage of missing Bid Amounts negatively affects data quality and weakens potential correlations in analysis.

## Visualizations:
- Draw.io diagram for data analytic platform for the City of Vancouver - Contracts


# Project 1: Descriptive Data Analysis

## Project Description:
This project analyzes the contracts awarded by the City of Vancouver group. The goal is to calculate the success rate of vendor bids and understand how effectively bids are being awarded.

## Project Title:
**Analyzing the Success Rate of Vendor Bids for the City of Vancouver Contracts**

## Objective:
Analyze how effective the bids proposed by vendors are by calculating the success rate of contracts awarded by the City of Vancouver group.

## Dataset:
The dataset includes 646 records of contract bids submitted by contractors or vendors for projects publicly listed by the City of Vancouver. Following are the features of the dataset:
- **Bid Type**: Request for Proposal
- **Bid Number**: Unique identifier for each bid
- **Bid Description**: Description of the contract
- **Award Date**: Date the contract was awarded
- **Vendor Name**: Name of the vendor or contractor
- **Bid Amount**: Monetary value of the bid
- **Awarded**: Status of the bid (Yes/No)

## Descriptive Analysis Question:
How effective are the bids proposed by vendors for contracts posted by the City of Vancouver group?

## Methodology:

### 1. Data Ingestion:
- Uploaded raw data to AWS S3 (cov-raw-roh) and organized it by year and quarter.
- Set S3 storage to Standard with lifecycle rules to move data to Glacier after 90 days.

### 2. Data Profiling:
- Utilized AWS Glue DataBrew to analyze the dataset and evaluate its quality.
- Identified missing values and duplicate rows, then stored the profiling results in dedicated subfolders.

### 3. Data Cleaning:
- Standardized column name by replacing the spaces with underscores.
- Deleted extra and unnecessary columns and duplicate rows.
- Stored cleaned data in CSV format for users and Parquet format for system processing.

### 4. Data Pipeline Design:
- Filtered the dataset to include only awarded contracts.
- Aggregated the total number of bids and awarded contracts, then calculated the success rate.
- Stored the results in the S3 (cov-trf-roh) bucket in CSV and Parquet formats.

## Tools & Technologies:
- **AWS S3**: Data storage
- **AWS Glue DataBrew**: Data profiling and cleaning
- **AWS Glue**: Data transformation and aggregation

## Deliverables:
- Raw and cleaned data are stored in CSV and Parquet formats in S3 buckets for efficient management.
- An AWS Glue pipeline is used for data transformation, aggregation, and calculating the success rate of awarded contracts.
- The data is then filtered and aggregated to determine the success rate of the contracts awarded.

## Insights & Findings:
- The overall bid success rate for awarded contracts is 31.5%.
- Certain vendors consistently achieve higher success rates, suggesting advantages in proposal quality or pricing.

## Visualizations:
- Draw.io diagram for data analytic platform for the City of Vancouver – Contracts


# Project 1: Diagnostic Analysis

## Project Description:
Diagnostic Analysis of Vendor Bids and Contract Awards at the City of Vancouver.

## Project Title:
**Investigating Data Quality, Completeness, and Structure of City of Vancouver Contract Bids Dataset**

## Objective:
The primary goal is to identify anomalies, missing data, inconsistencies, or patterns in the dataset that could impact its usability for generating data visualization, reports, and predictive modeling. Insights from this analysis will help uncover any issues that need to be addressed and guide the necessary data cleaning and transformation steps. This ensures the dataset is properly prepared and optimized for subsequent tasks, enabling accurate and effective analysis.

## Dataset:
The dataset contains details on contracts issued by the City of Vancouver. Following are the features of the dataset:
- **Bid Type**: Request for Proposal
- **Bid Number**: Unique identifier for each bid
- **Bid Description**: Description of the contract
- **Award Date**: Date the contract was awarded
- **Vendor Name**: Name of the vendor or contractor
- **Bid Amount**: Monetary value of the bid
- **Awarded**: Status of the bid (Yes/No)

## Methodology:

### Data Ingestion:
- Uploaded the raw dataset to Amazon S3 (bucket: cov-raw-roh).
- Organized data by year and quarter for structured storage and easy access.

### Data Profiling

#### Profiling Job Setup:
- Connected the dataset to AWS Glue DataBrew for profiling and quality assessment.
- Included all key fields:
  - **String Columns**: Bid_Number, Bid_Type, Vendor_Name, Awarded
  - **Numeric Columns**: Bid_Amount, Award_Date

#### Profiling Insights:
- **Data Quality**: Most columns contained valid values, but some columns contained missing values and duplicate entries.
- **Missing Values**:
  - **Bid_Amount**: 69% missing values, significantly impacting analysis.
  - **Vendor_Name**: A few missing entries were identified.
- **Duplicate Values**:
  - **Vendor_Name**: Unique identifiers, but some duplicates identified.

#### Storage of Results:
- Stored profiling results in Amazon S3 bucket (cov-trf-roh).

### Validation:
- **Missing Values**:
  - **Bid Amount**: 69% of the values were missing.
  - **Vendor Name**: There were a few missing entries, which could affect the analysis.
  - **Award Status**: There were no missing values identified in the Awarded field, ensuring completeness in terms of contract outcomes.
- **Duplicates**:
  - Duplicate entries were found in the Vendor Name field.

### Action Plan:
- To handle missing values, replace missing values with NULL values to indicate the absence of data.
- Remove all duplicate rows with the same entries.

### Outcome:
The diagnostic analysis identified key issues related to missing data, duplicates, and anomalies in the dataset. Addressing these issues will improve the dataset’s quality and ensure it is ready for accurate reporting, visualization, and predictive modeling.

## Tools & Technologies:
- **AWS Glue DataBrew**: Automated profiling to assess dataset quality.
- **Amazon S3**: Centralized storage for raw data and profiling results.

## Deliverables:
1. **Data Profiling Report**: Comprehensive report with data quality metrics, column statistics, and schema validation.
2. **Data Quality Insights**: Analysis of missing values, anomalies, and duplicates.
3. **Profiling Results Storage**: S3 Folder: Data profiling results are stored in the cov-trf-roh bucket.

## Visualizations:
- Draw.io diagram for data analytic platform for the City of Vancouver - Contracts


# Project 1: Data Wrangling

## Project Title:
**Data Wrangling for contracts awarded by the City of Vancouver**

## Objective: 
The goal of this project is to clean and structure data on vendor bids and contract awards at the City of Vancouver, improving its accuracy and usability for analyzing contract success rates, vendor performance, and bid trends.

## Background:
The data on vendor bids for contracts is often inconsistent, incomplete, or contains duplicates, making it hard to derive actionable insights. Effective data wrangling will ensure accuracy and completeness, enabling better analysis and decision-making in the contract award process.

## Dataset:
The dataset contains details on contracts issued by the City of Vancouver. Following are the features of the dataset:
- **Bid Type**: Request for Proposal
- **Bid Number**: Unique identifier for each bid
- **Bid Description**: Description of the contract
- **Award Date**: Date the contract was awarded
- **Vendor Name**: Name of the vendor or contractor
- **Bid Amount**: Monetary value of the bid
- **Awarded**: Status of the bid (Yes/No)

## Methodology:

### Data Collection & Preparation:
- **Data Ingestion**: Uploaded the raw dataset to Amazon S3 (bucket: cov-raw-roh).
- **Data Profiling**: Assess datasets for issues like missing values, duplicates, and inconsistent formats while documenting data types, formats, and discrepancies between datasets.

### Data Cleaning:
- Address missing values in key fields such as Bid Amount and Vendor Name, using imputation or exclusion depending on the significance of the data.
- Remove duplicates in the Bid Number field to ensure each bid is unique.
- Normalize categorical variables like Awarded Status to ensure consistency in values (e.g., "Yes"/"No" or "1"/"0").

### Data Transformation:
- Convert data types where necessary, such as converting Bid Amount to numeric values.
- Derive new features, such as Contract Award Rate (percentage of successful bids), to aid in the analysis of contract success.
- Aggregate data where needed, such as summarizing bid success by vendor.

## Tools and Technologies:
- **Amazon S3**: Centralized storage for raw and transformed data, ensuring easy access.
- **AWS Glue DataBrew**: Automates data cleaning and preparation for analysis.
- **AWS Glue**: Manages the data wrangling pipeline, automating transformation and consolidation.

## Deliverables:
- A cleaned dataset in CSV or Parquet format, ready for analysis.
- A report detailing the data wrangling process.
- Visualizations showing key insights and data quality checks.

## Visualizations:
- Draw.io diagram for data analytic platform for the City of Vancouver - Contracts


# Project 1: Data Quality Control

## Project Description:
Ensuring the contacts dataset meets quality standards by implementing control measures and audits to validate data accuracy and integrity.

## Project Title:
**Data Quality control for contracts published by the City of Vancouver.**

## Objective:
The objective of this project is to ensure that the contract posted by the City of Vancouver dataset meets high standards of quality, security, and observability. This involves implementing rigorous checks for data accuracy, completeness, and integrity, applying data security measures to protect sensitive information, and establishing observability solutions for real-time monitoring. These steps aim to deliver a reliable, secure, and trackable dataset for effective contract award analysis and informed decision-making by the City of Vancouver.

## Description:
High-quality data is vital for accurate analysis and decision-making. The dataset requires thorough checks to ensure its integrity and prevent errors.

## Scope: 
The scope of the dataset is to ensure that the dataset is reliable and usable for analysis and decision-making.

## Dataset:
The dataset contains details on contracts issued by the City of Vancouver. Following are the features of the dataset:
- **Bid Type**: Request for Proposal
- **Bid Number**: Unique identifier for each bid
- **Bid Description**: Description of the contract
- **Award Date**: Date the contract was awarded
- **Vendor Name**: Name of the vendor or contractor
- **Bid Amount**: Monetary value of the bid
- **Awarded**: Status of the bid (Yes/No)

## Methodology:

### Data Enriching:
- **AWS Glue**:
  - Created a data pipeline named “cov-cont-QPC-roh” to process clean data from the transform S3 bucket.
  - Enriched the dataset by adding Bid Amount to analyze the relationship between bid amounts and contract success rates.
  - Generated a data catalog using AWS Glue Crawlers to convert datasets into tables for easy querying.

### Data Governance:
- **Completeness and Uniqueness Rules**:
  - Applied data quality rules in AWS Glue DataBrew:
    - **Completeness Rule**: Ensured the Bid Number field was 95% complete.
    - **Uniqueness Rule**: Ensured the Vendor Name field was 99% unique.
- **Conditional Routing**:
  - Separated the results into “Passed” and “Failed” datasets using a conditional router node.
  - Stored the results in designated S3 folders:
    - Passed Results: `s3://cov-trf-roh/Contracts/data-quality/Passed/`
    - Failed Results: `s3://cov-trf-roh/Contracts/data-quality/Failed/`

### Data Security:
- **AWS KMS (Key Management Service)**:
  - Encrypted S3 buckets using customer-managed encryption keys to protect sensitive data.
  - Enabled bucket versioning to maintain data integrity and recover previous versions if necessary.
- **Cross-Region Replication**:
  - Implemented cross-region replication with encryption to ensure data availability and disaster recovery.

### Data Observability:
- **Amazon CloudWatch**:
  - Created dashboards to monitor key metrics, including:
    - Bucket Sizes and the Number of Objects in S3 buckets (Raw, Transformed, and Curated).
    - Billing Gauges to track costs associated with storage and compute resources.

## Tools and Technologies:
- **AWS Glue**: ETL data pipeline creation and data cataloging.
- **AWS S3**: Data storage and security measures.
- **AWS KMS**: Data encryption.
- **Amazon CloudWatch**: Observability and monitoring.

## Deliverables:
- A quality-checked dataset stored in AWS S3, with passed and failed results for contract analysis.
- Data governance rules and audit reports detailing the completeness and uniqueness checks.
- A secure data pipeline that ensures data integrity and privacy.
- Real-time observability and monitoring dashboards.

## Visualizations:
- Draw.io diagram for data analytic platform for the City of Vancouver - Contracts
